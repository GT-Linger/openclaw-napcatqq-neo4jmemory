import type { OpenClawConfig } from "../config/config.js";
import type { RuntimeEnv } from "../runtime.js";
import type { WizardPrompter } from "../wizard/prompts.js";
import {
  getTemplateNames,
  getTemplateById,
  createSubagentFromTemplate,
} from "../agents/subagent-templates.js";
import { listSubagents, addSubagent } from "../agents/subagent-manager.js";
import { enhanceSubagentConfig, canEnhancePersonality } from "../agents/subagent-personality-enhancer.js";
import { createSubagentWorkspaceFromConfig } from "../agents/subagent-workspace.js";
import type { SubagentConfig, ModelEndpoint } from "../agents/subagent-config.js";
import type { VllmServerConfig } from "../agents/subagent-vllm-config.js";
import type { SglangServerConfig } from "../agents/sglang-manager.js";
import {
  loadVllmModelsConfig,
  saveVllmModelsConfig,
} from "../agents/model-service-integration.js";

export interface MainAgentEndpoint {
  baseUrl: string;
  model: string;
  apiKey?: string;
}

async function checkEndpointAvailable(baseUrl: string): Promise<boolean> {
  try {
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), 3000);

    const response = await fetch(baseUrl.replace("/v1", "") + "/models", {
      method: "GET",
      signal: controller.signal,
    });

    clearTimeout(timeoutId);
    return response.ok;
  } catch {
    return false;
  }
}

export type MainAgentEndpointStatus = {
  available: boolean;
  endpoint: MainAgentEndpoint | undefined;
  reason?: string;
};

export async function checkMainAgentEndpoint(cfg: OpenClawConfig): Promise<MainAgentEndpointStatus> {
  const endpoint = getMainAgentEndpointFromConfig(cfg);
  if (!endpoint) {
    return {
      available: false,
      endpoint: undefined,
      reason: "æœªé…ç½®ä¸»æ™ºèƒ½ä½“æ¨¡å‹",
    };
  }

  const isAvailable = await checkEndpointAvailable(endpoint.baseUrl);
  if (!isAvailable) {
    return {
      available: false,
      endpoint,
      reason: "ä¸»æ™ºèƒ½ä½“æ¨¡å‹æœåŠ¡æœªè¿è¡Œ",
    };
  }

  return {
    available: true,
    endpoint,
  };
}

function getMainAgentEndpointFromConfig(cfg: OpenClawConfig): MainAgentEndpoint | undefined {
  const modelConfig = cfg.agents?.defaults?.model;
  if (!modelConfig) {
    return undefined;
  }

  let primaryModel = "";
  if (typeof modelConfig === "string") {
    primaryModel = modelConfig;
  } else if (modelConfig.primary) {
    primaryModel = modelConfig.primary;
  }

  if (!primaryModel) {
    return undefined;
  }

  const [provider, model] = primaryModel.includes("/")
    ? primaryModel.split("/")
    : [undefined, primaryModel];

  let baseUrl = "https://api.openai.com/v1";
  if (provider === "anthropic") {
    baseUrl = "https://api.anthropic.com/v1";
  } else if (provider === "ollama") {
    baseUrl = "http://localhost:11434/v1";
  } else if (provider === "vllm") {
    baseUrl = "http://localhost:8000/v1";
  } else if (provider === "sglang") {
    baseUrl = "http://localhost:8000/v1";
  }

  return {
    baseUrl,
    model: model || primaryModel,
  };
}

async function promptModelProvider(prompter: WizardPrompter): Promise<string> {
  const choice = await prompter.select({
    message: "é€‰æ‹©æ¨¡å‹ä¾›åº”å•†",
    options: [
      { value: "vllm", label: "vLLM", hint: "é«˜æ€§èƒ½ LLM æ¨ç†æœåŠ¡" },
      { value: "ollama", label: "Ollama", hint: "æœ¬åœ° LLM æ¨ç†æ¡†æ¶" },
      { value: "sglang", label: "SGLang", hint: "å¿«é€Ÿ LLM æ¨ç†å¼•æ“" },
      { value: "openai", label: "OpenAI API", hint: "OpenAI GPT ç³»åˆ—æ¨¡å‹" },
      { value: "anthropic", label: "Anthropic API", hint: "Claude ç³»åˆ—æ¨¡å‹" },
      { value: "custom", label: "è‡ªå®šä¹‰ API", hint: "å…¼å®¹ OpenAI çš„è‡ªå®šä¹‰ API" },
    ],
  });
  return choice as string;
}

async function promptServerLocation(prompter: WizardPrompter): Promise<"local" | "remote"> {
  const choice = await prompter.select({
    message: "vLLM è¿è¡Œä½ç½®",
    options: [
      { value: "local", label: "æœ¬åœ°æœåŠ¡å™¨", hint: "vLLM è¿è¡Œåœ¨æœ¬æœº" },
      { value: "remote", label: "è¿œç¨‹æœåŠ¡å™¨", hint: "vLLM è¿è¡Œåœ¨å…¶ä»–æœºå™¨ä¸Šï¼Œéœ€è¦ SSH è¿æ¥" },
    ],
  });
  return choice as "local" | "remote";
}

async function promptDeploymentMethod(prompter: WizardPrompter): Promise<"command" | "docker"> {
  const choice = await prompter.select({
    message: "vLLM éƒ¨ç½²æ–¹å¼",
    options: [
      { value: "command", label: "å‘½ä»¤è¡Œ", hint: "ç›´æ¥è¿è¡Œ vllm å‘½ä»¤" },
      { value: "docker", label: "Docker å®¹å™¨", hint: "ä½¿ç”¨ Docker å®¹å™¨è¿è¡Œ vLLM" },
    ],
  });
  return choice as "command" | "docker";
}

interface DockerConfigInput {
  image: string;
  containerName?: string;
  gpuDevices?: string;
  volumes?: string[];
  envVars?: Record<string, string>;
  extraArgs?: string;
}

async function promptDockerConfig(prompter: WizardPrompter): Promise<DockerConfigInput> {
  const image = String(await prompter.text({
    message: "vLLM Docker é•œåƒ",
    initialValue: "vllm/vllm:latest",
    placeholder: "ä¾‹å¦‚: vllm/vllm:latest æˆ– vllm/vllm:0.6.3post1-cu124",
  }));

  const containerName = String(await prompter.text({
    message: "å®¹å™¨åç§°ï¼ˆå¯é€‰ï¼Œç•™ç©ºè‡ªåŠ¨ç”Ÿæˆï¼‰",
    placeholder: "ä¾‹å¦‚: vllm-coder",
  }));

  const useGpu = await prompter.confirm({
    message: "æ˜¯å¦å¯ç”¨ GPU æ”¯æŒï¼Ÿï¼ˆLLM æ¨ç†éœ€è¦ GPUï¼‰",
    initialValue: true,
  });

  let gpuDevices: string | undefined;
  if (useGpu) {
    const gpuChoice = await prompter.select({
      message: "GPU è®¾å¤‡é€‰æ‹©",
      options: [
        { value: "all", label: "æ‰€æœ‰ GPU", hint: "ä½¿ç”¨æœåŠ¡å™¨ä¸Šæ‰€æœ‰ GPU" },
        { value: "0", label: "GPU 0", hint: "ä»…ä½¿ç”¨ç¬¬ä¸€ä¸ª GPU" },
        { value: "0,1", label: "GPU 0,1", hint: "ä½¿ç”¨å‰ä¸¤ä¸ª GPU" },
        { value: "custom", label: "è‡ªå®šä¹‰", hint: "æ‰‹åŠ¨è¾“å…¥ GPU ID" },
      ],
    });

    if (gpuChoice === "custom") {
      gpuDevices = String(await prompter.text({
        message: "è¾“å…¥ GPU è®¾å¤‡ IDï¼ˆé€—å·åˆ†éš”ï¼‰",
        placeholder: "ä¾‹å¦‚: 0,1,2",
      }));
    } else {
      gpuDevices = gpuChoice;
    }
  }

  const useVolumes = await prompter.confirm({
    message: "æ˜¯å¦éœ€è¦åŠ è½½æœ¬åœ°æ¨¡å‹æ–‡ä»¶ï¼Ÿï¼ˆå¦‚æœä½¿ç”¨ HuggingFace æ¨¡å‹ ID åˆ™ä¸éœ€è¦ï¼‰",
    initialValue: false,
  });

  let volumes: string[] | undefined;
  if (useVolumes) {
    const volumesInput = String(await prompter.text({
      message: "å·æŒ‚è½½ï¼ˆä¸»æœºè·¯å¾„:å®¹å™¨è·¯å¾„ï¼Œå¤šä¸ªç”¨é€—å·åˆ†éš”ï¼‰",
      placeholder: "ä¾‹å¦‚: /local/models:/models,/data:/data",
    }));
    volumes = volumesInput.split(",").map(v => v.trim()).filter(Boolean);
  }

  const extraArgs = String(await prompter.text({
    message: "é¢å¤– Docker å‚æ•°ï¼ˆå¯é€‰ï¼‰",
    placeholder: "ä¾‹å¦‚: --shm-size=16g",
  }));

  return {
    image: String(image).trim(),
    containerName: containerName.trim() || undefined,
    gpuDevices,
    volumes: volumes?.length ? volumes : undefined,
    extraArgs: extraArgs.trim() || undefined,
  };
}

async function promptRemoteServerConfig(prompter: WizardPrompter, useDocker: boolean = false): Promise<VllmServerConfig> {
  const remoteHost = String(await prompter.text({
    message: "è¿œç¨‹æœåŠ¡å™¨åœ°å€ (IP æˆ–åŸŸå)",
    placeholder: "ä¾‹å¦‚: 192.168.1.100",
  }));

  const remotePortStr = String(await prompter.text({
    message: "vLLM æœåŠ¡ç«¯å£",
    initialValue: "8000",
  }));
  const remotePort = parseInt(remotePortStr, 10) || 8000;

  const useSsh = await prompter.confirm({
    message: "æ˜¯å¦é€šè¿‡ SSH å¯åŠ¨/åœæ­¢è¿œç¨‹ vLLMï¼Ÿ",
    initialValue: true,
  });

  if (!useSsh) {
    return {
      type: "remote",
      host: remoteHost.trim(),
      port: remotePort,
    };
  }

  const sshHost = String(await prompter.text({
    message: "SSH æœåŠ¡å™¨åœ°å€ (ç•™ç©ºåˆ™ä½¿ç”¨è¿œç¨‹æœåŠ¡å™¨åœ°å€)",
    initialValue: remoteHost.trim(),
  }));

  const sshPortStr = String(await prompter.text({
    message: "SSH ç«¯å£",
    initialValue: "22",
  }));
  const sshPort = parseInt(sshPortStr, 10) || 22;

  const sshUsername = String(await prompter.text({
    message: "SSH ç”¨æˆ·å",
    initialValue: "root",
  }));

  const authMethod = await prompter.select({
    message: "SSH è®¤è¯æ–¹å¼",
    options: [
      { value: "key", label: "ç§é’¥æ–‡ä»¶", hint: "ä½¿ç”¨ SSH ç§é’¥è®¤è¯ï¼ˆæ¨èï¼‰" },
      { value: "password", label: "å¯†ç ", hint: "ä½¿ç”¨å¯†ç è®¤è¯ï¼ˆå®‰å…¨æ€§è¾ƒä½ï¼‰" },
    ],
  });

  let privateKeyPath: string | undefined;
  let password: string | undefined;

  if (authMethod === "key") {
    const homeDir = process.env.HOME || "";
    privateKeyPath = String(await prompter.text({
      message: "SSH ç§é’¥è·¯å¾„",
      initialValue: `${homeDir}/.ssh/id_rsa`,
    }));
  } else {
    password = String(await prompter.text({
      message: "SSH å¯†ç ",
    }));
  }

  return {
    type: "remote",
    host: remoteHost.trim(),
    port: remotePort,
    ssh: {
      enabled: true,
      host: sshHost.trim() || remoteHost.trim(),
      port: sshPort,
      username: sshUsername.trim(),
      privateKeyPath: privateKeyPath?.trim(),
      password: password?.trim(),
    },
  };
}

async function promptDockerServerConfig(prompter: WizardPrompter, isRemote: boolean = false): Promise<VllmServerConfig> {
  if (isRemote) {
    await prompter.note(
      "ç¡®ä¿è¿œç¨‹æœåŠ¡å™¨å·²å®‰è£… Docker å¹¶å…·æœ‰ GPU æ”¯æŒã€‚å¯è¿è¡Œ 'docker run --gpus all nvidia/cuda:12.1-base nvidia-smi' æµ‹è¯•ã€‚",
      "å‰ææ¡ä»¶",
    );
  }

  let host = "127.0.0.1";
  let ssh: VllmServerConfig["ssh"] | undefined;

  if (isRemote) {
    const sshHost = String(await prompter.text({
      message: "SSH æœåŠ¡å™¨åœ°å€ (IP æˆ–åŸŸå)",
      placeholder: "ä¾‹å¦‚: 192.168.1.100",
    }));

    const sshPortStr = String(await prompter.text({
      message: "SSH ç«¯å£",
      initialValue: "22",
    }));
    const sshPort = parseInt(sshPortStr, 10) || 22;

    const sshUsername = String(await prompter.text({
      message: "SSH ç”¨æˆ·å",
      initialValue: "root",
    }));

    const authMethod = await prompter.select({
      message: "SSH è®¤è¯æ–¹å¼",
      options: [
        { value: "key", label: "ç§é’¥æ–‡ä»¶", hint: "ä½¿ç”¨ SSH ç§é’¥è®¤è¯ï¼ˆæ¨èï¼‰" },
        { value: "password", label: "å¯†ç ", hint: "ä½¿ç”¨å¯†ç è®¤è¯ï¼ˆå®‰å…¨æ€§è¾ƒä½ï¼‰" },
      ],
    });

    let privateKeyPath: string | undefined;
    let password: string | undefined;

    if (authMethod === "key") {
      const homeDir = process.env.HOME || "";
      privateKeyPath = String(await prompter.text({
        message: "SSH ç§é’¥è·¯å¾„",
        initialValue: `${homeDir}/.ssh/id_rsa`,
      }));
    } else {
      password = String(await prompter.text({
        message: "SSH å¯†ç ",
      }));
    }

    host = String(await prompter.text({
      message: "è¿œç¨‹æœåŠ¡å™¨åœ°å€ï¼ˆDocker ä¸»æœºï¼‰",
      placeholder: "ä¾‹å¦‚: 192.168.1.100",
    }));

    ssh = {
      enabled: true,
      host: sshHost.trim(),
      port: sshPort,
      username: sshUsername.trim(),
      privateKeyPath: privateKeyPath?.trim(),
      password: password?.trim(),
    };
  } else {
    host = String(await prompter.text({
      message: "Docker ä¸»æœºåœ°å€",
      initialValue: "127.0.0.1",
    }));
  }

  const dockerConfig = await promptDockerConfig(prompter);

  return {
    type: "docker",
    host: host.trim(),
    port: 8000,
    ssh,
    docker: {
      enabled: true,
      image: dockerConfig.image,
      containerName: dockerConfig.containerName,
      gpuDevices: dockerConfig.gpuDevices,
      volumes: dockerConfig.volumes,
      extraArgs: dockerConfig.extraArgs,
    },
  };
}

async function promptRemoteServerConfigSglang(prompter: WizardPrompter): Promise<SglangServerConfig> {
  const remoteHost = String(await prompter.text({
    message: "è¿œç¨‹æœåŠ¡å™¨åœ°å€ (IP æˆ–åŸŸå)",
    placeholder: "ä¾‹å¦‚: 192.168.1.100",
  }));

  const remotePortStr = String(await prompter.text({
    message: "SGLang æœåŠ¡ç«¯å£",
    initialValue: "9000",
  }));
  const remotePort = parseInt(remotePortStr, 10) || 9000;

  const useSsh = await prompter.confirm({
    message: "æ˜¯å¦é€šè¿‡ SSH å¯åŠ¨/åœæ­¢è¿œç¨‹ SGLangï¼Ÿ",
    initialValue: true,
  });

  let ssh: SglangServerConfig["ssh"] | undefined;
  let host = remoteHost.trim();

  if (useSsh) {
    const sshHost = String(await prompter.text({
      message: "SSH æœåŠ¡å™¨åœ°å€ (ç•™ç©ºåˆ™ä½¿ç”¨è¿œç¨‹æœåŠ¡å™¨åœ°å€)",
      initialValue: remoteHost.trim(),
    }));

    const sshPortStr = String(await prompter.text({
      message: "SSH ç«¯å£",
      initialValue: "22",
    }));
    const sshPort = parseInt(sshPortStr, 10) || 22;

    const sshUsername = String(await prompter.text({
      message: "SSH ç”¨æˆ·å",
      initialValue: "root",
    }));

    const usePrivateKey = await prompter.confirm({
      message: "æ˜¯å¦ä½¿ç”¨ SSH ç§é’¥è®¤è¯ï¼Ÿ",
      initialValue: true,
    });

    let privateKeyPath: string | undefined;
    let password: string | undefined;

    if (usePrivateKey) {
      const homeDir = process.env.HOME || process.env.USERPROFILE || "";
      privateKeyPath = String(await prompter.text({
        message: "SSH ç§é’¥è·¯å¾„",
        initialValue: `${homeDir}/.ssh/id_rsa`,
      }));
    } else {
      password = String(await prompter.text({
        message: "SSH å¯†ç ",
      }));
    }

    ssh = {
      host: sshHost.trim(),
      port: sshPort,
      username: sshUsername.trim(),
      privateKeyPath: privateKeyPath?.trim(),
    };
  }

  return {
    type: "remote",
    host: host.trim(),
    port: remotePort,
    ssh,
  };
}

async function promptDockerServerConfigSglang(prompter: WizardPrompter, isRemote: boolean = false): Promise<SglangServerConfig> {
  if (isRemote) {
    await prompter.note(
      "ç¡®ä¿è¿œç¨‹æœåŠ¡å™¨å·²å®‰è£… Docker å¹¶å…·æœ‰ GPU æ”¯æŒã€‚å¯è¿è¡Œ 'docker run --gpus all nvidia/cuda:12.1-base nvidia-smi' æµ‹è¯•ã€‚",
      "å‰ææ¡ä»¶",
    );
  }

  let host = "127.0.0.1";
  let ssh: SglangServerConfig["ssh"] | undefined;

  if (isRemote) {
    const sshHost = String(await prompter.text({
      message: "SSH æœåŠ¡å™¨åœ°å€",
      placeholder: "ä¾‹å¦‚: 192.168.1.100",
    }));

    const sshPortStr = String(await prompter.text({
      message: "SSH ç«¯å£",
      initialValue: "22",
    }));
    const sshPort = parseInt(sshPortStr, 10) || 22;

    const sshUsername = String(await prompter.text({
      message: "SSH ç”¨æˆ·å",
      initialValue: "root",
    }));

    const usePrivateKey = await prompter.confirm({
      message: "æ˜¯å¦ä½¿ç”¨ SSH ç§é’¥è®¤è¯ï¼Ÿ",
      initialValue: true,
    });

    let privateKeyPath: string | undefined;

    if (usePrivateKey) {
      const homeDir = process.env.HOME || process.env.USERPROFILE || "";
      privateKeyPath = String(await prompter.text({
        message: "SSH ç§é’¥è·¯å¾„",
        initialValue: `${homeDir}/.ssh/id_rsa`,
      }));
    }

    host = String(await prompter.text({
      message: "è¿œç¨‹æœåŠ¡å™¨åœ°å€ï¼ˆDocker ä¸»æœºï¼‰",
      placeholder: "ä¾‹å¦‚: 192.168.1.100",
    }));

    ssh = {
      host: sshHost.trim(),
      port: sshPort,
      username: sshUsername.trim(),
      privateKeyPath: privateKeyPath?.trim(),
    };
  } else {
    host = String(await prompter.text({
      message: "Docker ä¸»æœºåœ°å€",
      initialValue: "127.0.0.1",
    }));
  }

  const dockerConfig = await promptDockerConfig(prompter);

  return {
    type: "docker",
    host: host.trim(),
    port: 9000,
    ssh,
    docker: {
      enabled: true,
      image: dockerConfig.image,
      containerName: dockerConfig.containerName,
      gpuDevices: dockerConfig.gpuDevices,
      volumes: dockerConfig.volumes,
      extraArgs: dockerConfig.extraArgs,
    },
  };
}

async function promptBaseUrl(prompter: WizardPrompter, provider: string): Promise<string> {
  const defaultUrls: Record<string, string> = {
    vllm: "http://localhost:8000/v1",
    sglang: "http://localhost:8000/v1",
    ollama: "http://localhost:11434",
    openai: "https://api.openai.com/v1",
    anthropic: "https://api.anthropic.com/v1",
    custom: "http://localhost:8000/v1",
  };

  const url = await prompter.text({
    message: "API åŸºç¡€ URL",
    initialValue: defaultUrls[provider] || "http://localhost:8000/v1",
  });
  return String(url || defaultUrls[provider]);
}

async function promptModelName(prompter: WizardPrompter, provider: string): Promise<string> {
  const defaultModels: Record<string, string> = {
    vllm: "qwen2.5-7b-instruct",
    sglang: "qwen2.5-7b-instruct",
    ollama: "llama3.1",
    openai: "gpt-4o-mini",
    anthropic: "claude-3-5-haiku-20241022",
    custom: "gpt-4o-mini",
  };

  const model = await prompter.text({
    message: "æ¨¡å‹åç§°",
    initialValue: defaultModels[provider],
  });
  return String(model || defaultModels[provider]);
}

interface GpuMemoryConfig {
  gpuMemoryUtilization?: number;
  maxModelLen?: number;
}

async function promptGpuMemoryConfig(prompter: WizardPrompter): Promise<GpuMemoryConfig> {
  const enableLimit = await prompter.confirm({
    message: "æ˜¯å¦éœ€è¦é™åˆ¶ GPU æ˜¾å­˜ä½¿ç”¨ï¼Ÿï¼ˆé¿å… OOMï¼‰",
    initialValue: false,
  });

  if (!enableLimit) {
    return {};
  }

  const gpuMemoryUtilization = Number(await prompter.text({
    message: "GPU æ˜¾å­˜åˆ©ç”¨ç‡ (0.0-1.0)",
    placeholder: "ä¾‹å¦‚: 0.9 è¡¨ç¤ºä½¿ç”¨ 90% æ˜¾å­˜",
    initialValue: "0.9",
  }));

  const hasMaxModelLen = await prompter.confirm({
    message: "æ˜¯å¦é™åˆ¶æ¨¡å‹æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ï¼Ÿï¼ˆå¯å‡å°‘æ˜¾å­˜ï¼‰",
    initialValue: false,
  });

  let maxModelLen: number | undefined;
  if (hasMaxModelLen) {
    maxModelLen = Number(await prompter.text({
      message: "æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦",
      placeholder: "ä¾‹å¦‚: 32768",
      initialValue: "32768",
    }));
  }

  return {
    gpuMemoryUtilization: gpuMemoryUtilization || 0.9,
    maxModelLen,
  };
}

async function promptSubagentName(prompter: WizardPrompter): Promise<string> {
  const name = await prompter.text({
    message: "å­æ™ºèƒ½ä½“åç§°",
    placeholder: "ä¾‹å¦‚ï¼šä»£ç åŠ©æ‰‹",
  });
  return String(name).trim();
}

async function promptSubagentLabel(prompter: WizardPrompter): Promise<string> {
  const label = await prompter.text({
    message: "å­æ™ºèƒ½ä½“æ ‡è¯†ç¬¦ (label)",
    placeholder: "ä¾‹å¦‚ï¼šcoding-agent",
  });
  return String(label).trim().toLowerCase().replace(/\s+/g, "-");
}

async function promptSubagentDescription(prompter: WizardPrompter): Promise<string> {
  const description = await prompter.text({
    message: "å­æ™ºèƒ½ä½“å·¥ä½œå†…å®¹æè¿°",
    placeholder: "ä¾‹å¦‚ï¼šå¸®æˆ‘å†™ä»£ç ã€è°ƒè¯•bug",
  });
  return String(description).trim();
}

async function promptSubagentCount(prompter: WizardPrompter): Promise<number> {
  const countStr = await prompter.text({
    message: "åˆ›å»ºæ•°é‡",
    initialValue: "1",
  });
  const count = parseInt(String(countStr).trim(), 10);
  return isNaN(count) || count < 1 ? 1 : count > 10 ? 10 : count;
}

async function handleAIEnhancement(
  prompter: WizardPrompter,
  cfg: OpenClawConfig,
  config: SubagentConfig,
): Promise<SubagentConfig> {
  const endpointStatus = await checkMainAgentEndpoint(cfg);

  if (!endpointStatus.available) {
    const skipEnhance = await prompter.confirm({
      message: `AI å¢å¼ºè·³è¿‡ï¼š${endpointStatus.reason}ã€‚æ˜¯å¦è·³è¿‡äººæ ¼å¢å¼ºï¼Ÿ`,
      initialValue: true,
    });

    if (skipEnhance) {
      await prompter.note(
        "å·²è·³è¿‡ AI å¢å¼ºã€‚å¯åœ¨ä¸»æ™ºèƒ½ä½“æ¨¡å‹æœåŠ¡å¯åŠ¨åï¼Œé€šè¿‡ 'openclaw subagent enhance <id>' æ‰‹åŠ¨å¢å¼ºã€‚",
        "è·³è¿‡"
      );
      return config;
    } else {
      await prompter.note("è¯·å…ˆå¯åŠ¨ä¸»æ™ºèƒ½ä½“æ¨¡å‹æœåŠ¡ï¼Œæˆ–é€‰æ‹©å…¶ä»–æ¨¡å‹ä¾›åº”å•†ã€‚", "æç¤º");
      return config;
    }
  }

  const shouldEnhance = await prompter.confirm({
    message: "ä½¿ç”¨ AI å¢å¼ºäººæ ¼æè¿°ï¼Ÿï¼ˆéœ€è¦ä¸»æ™ºèƒ½ä½“æ¨¡å‹ï¼‰",
    initialValue: true,
  });

  if (shouldEnhance) {
    await prompter.note("æ­£åœ¨ä½¿ç”¨ AI å¢å¼ºäººæ ¼æè¿°...", "è¯·ç¨å€™");
    config = await enhanceSubagentConfig(config, endpointStatus.endpoint!);
    if (config.personality?.enhanced) {
      await prompter.note(
        `å¢å¼ºåçš„äººæ ¼æè¿°ï¼š\n${config.personality.enhanced.slice(0, 200)}...`,
        "äººæ ¼å·²å¢å¼º"
      );
    }
  }

  return config;
}

export async function setupAgents(
  cfg: OpenClawConfig,
  runtime: RuntimeEnv,
  prompter: WizardPrompter,
): Promise<OpenClawConfig> {
  const subagents = listSubagents();
  const hasSubagents = subagents.length > 0;

  const statusLines: string[] = [
    `å½“å‰å­æ™ºèƒ½ä½“æ•°é‡: ${subagents.length || 0}`,
    "",
    "å­æ™ºèƒ½ä½“åˆ—è¡¨:",
  ];

  if (subagents.length > 0) {
    for (const sa of subagents) {
      const modelInfo = `${sa.model.endpoint.provider} - ${sa.model.endpoint.model}`;
      statusLines.push(`  - ${sa.name} [${sa.id}]`);
      statusLines.push(`    æ¨¡å‹: ${modelInfo}`);
      if (sa.personality?.enhanced) {
        statusLines.push(`    äººæ ¼: âœ… å·²å¢å¼º`);
      }
    }
  } else {
    statusLines.push("  - æš‚æ— å­æ™ºèƒ½ä½“");
  }

  statusLines.push("");
  statusLines.push("vLLM å­æ™ºèƒ½ä½“ç³»ç»Ÿå…è®¸ä¸åŒä»»åŠ¡ç”±ä¸“ä¸š AI æ¨¡å‹å¤„ç†ã€‚");
  statusLines.push("ä¾‹å¦‚ï¼šç¼–ç¨‹ä»»åŠ¡ç”±ä»£ç æ¨¡å‹å¤„ç†ï¼Œæ•°å­¦ä»»åŠ¡ç”±æ•°å­¦æ¨¡å‹å¤„ç†ã€‚");
  statusLines.push("æ¯ä¸ªå­æ™ºèƒ½ä½“æœ‰ç‹¬ç«‹çš„æ¨¡å‹æœåŠ¡ï¼ŒæŒ‰éœ€å¯åŠ¨å’Œåœæ­¢ã€‚");
  statusLines.push("æ”¯æŒä¾›åº”å•†ï¼švLLMã€Ollamaã€SGLangã€OpenAIã€Anthropicã€è‡ªå®šä¹‰ APIã€‚");

  await prompter.note(statusLines.join("\n"), "vLLM å­æ™ºèƒ½ä½“é…ç½®");

  const shouldConfigure = await prompter.confirm({
    message: "é…ç½® vLLM å­æ™ºèƒ½ä½“ï¼Ÿ",
    initialValue: !hasSubagents,
  });

  if (!shouldConfigure) {
    return cfg;
  }

  let continueConfiguring = true;

  while (continueConfiguring) {
    const action = await prompter.select({
      message: "é€‰æ‹©æ“ä½œ",
      options: [
        { value: "create", label: "åˆ›å»ºå­æ™ºèƒ½ä½“", hint: "ä»æ¨¡æ¿é€‰æ‹©æˆ–æ‰‹åŠ¨å¡«å†™" },
        { value: "list", label: "æŸ¥çœ‹åˆ—è¡¨", hint: "æŸ¥çœ‹å·²åˆ›å»ºçš„å­æ™ºèƒ½ä½“" },
        { value: "done", label: "å®Œæˆ", hint: "é€€å‡ºé…ç½®" },
      ],
    });

    if (action === "list") {
      const currentList = listSubagents();
      if (currentList.length === 0) {
        await prompter.note("æš‚æ— å­æ™ºèƒ½ä½“", "åˆ—è¡¨");
      } else {
        const listLines: string[] = [];
        for (const sa of currentList) {
          listLines.push(`ğŸ“Œ ${sa.name} (${sa.id})`);
          listLines.push(`   æè¿°: ${sa.description}`);
          listLines.push(`   æ¨¡å‹: ${sa.model.endpoint.provider} - ${sa.model.endpoint.model}`);
          listLines.push("");
        }
        await prompter.note(listLines.join("\n"), "å­æ™ºèƒ½ä½“åˆ—è¡¨");
      }
      continue;
    }

    if (action === "done") {
      continueConfiguring = false;
      break;
    }

    if (action === "create") {
      let continueCreating = true;

      while (continueCreating) {
        const createType = await prompter.select({
          message: "åˆ›å»ºæ–¹å¼",
          options: [
            { value: "template", label: "ä»æ¨¡æ¿é€‰æ‹©", hint: "åŸºäºé¢„ç½®æ¨¡æ¿åˆ›å»º" },
            { value: "manual", label: "æ‰‹åŠ¨å¡«å†™", hint: "å®Œå…¨è‡ªå®šä¹‰é…ç½®" },
          ],
        });

        let config: SubagentConfig;

        if (createType === "template") {
          const templateOptions = getTemplateNames().map((t) => ({
            value: t.id,
            label: t.name,
            hint: t.description.slice(0, 40) + "...",
          }));

          const selectedId = await prompter.select({
            message: "é€‰æ‹©å­æ™ºèƒ½ä½“æ¨¡æ¿",
            options: templateOptions,
          });

          const template = getTemplateById(selectedId);
          if (!template) {
            await prompter.note("æ¨¡æ¿ä¸å­˜åœ¨", "é”™è¯¯");
            break;
          }

          const name = await promptSubagentName(prompter);
          const label = await promptSubagentLabel(prompter);
          const description = await promptSubagentDescription(prompter);

          config = createSubagentFromTemplate(template, {
            id: label || `subagent-${Date.now()}`,
            name: name || template.name,
            description: description || template.exampleDescription,
          });

          const modifyModel = await prompter.confirm({
            message: "æ˜¯å¦ä¿®æ”¹æ¨¡å‹é…ç½®ï¼Ÿï¼ˆå½“å‰ä½¿ç”¨æ¨¡æ¿é»˜è®¤æ¨¡å‹ï¼‰",
            initialValue: false,
          });

          if (modifyModel) {
            const provider = await promptModelProvider(prompter);
            let baseUrl: string;
            let server: VllmServerConfig | SglangServerConfig | undefined;
            
            if (provider === "vllm" || provider === "sglang") {
              const serverLocation = await promptServerLocation(prompter);
              const deploymentMethod = await promptDeploymentMethod(prompter);
              
              if (serverLocation === "local" && deploymentMethod === "command") {
                baseUrl = await promptBaseUrl(prompter, provider);
              } else if (serverLocation === "remote" && deploymentMethod === "command") {
                server = provider === "vllm" 
                  ? await promptRemoteServerConfig(prompter, false)
                  : await promptRemoteServerConfigSglang(prompter);
                baseUrl = `http://${(server as VllmServerConfig).host}:${(server as VllmServerConfig).port}/v1`;
              } else if (deploymentMethod === "docker") {
                server = provider === "vllm" 
                  ? await promptDockerServerConfig(prompter, serverLocation === "remote")
                  : await promptDockerServerConfigSglang(prompter, serverLocation === "remote");
                baseUrl = `http://${(server as VllmServerConfig).host}:${(server as VllmServerConfig).port}/v1`;
              } else {
                baseUrl = await promptBaseUrl(prompter, provider);
              }
            } else {
              baseUrl = await promptBaseUrl(prompter, provider);
            }
            
            const model = await promptModelName(prompter, provider);

            let gpuMemoryConfig: GpuMemoryConfig = {};
            if (provider === "vllm" || provider === "sglang") {
              gpuMemoryConfig = await promptGpuMemoryConfig(prompter);
            }

            config.model.endpoint = {
              provider: provider as any,
              baseUrl,
              model,
              server: server as any,
              gpuMemoryUtilization: gpuMemoryConfig.gpuMemoryUtilization,
              maxModelLen: gpuMemoryConfig.maxModelLen,
            };
          }
        } else {
          const name = await promptSubagentName(prompter);
          const label = await promptSubagentLabel(prompter);
          const description = await promptSubagentDescription(prompter);

          const provider = await promptModelProvider(prompter);
          let baseUrl: string;
          let server: VllmServerConfig | SglangServerConfig | undefined;
          
          if (provider === "vllm" || provider === "sglang") {
            const serverLocation = await promptServerLocation(prompter);
            const deploymentMethod = await promptDeploymentMethod(prompter);
            
            if (serverLocation === "local" && deploymentMethod === "command") {
              baseUrl = await promptBaseUrl(prompter, provider);
            } else if (serverLocation === "remote" && deploymentMethod === "command") {
              server = provider === "vllm" 
                ? await promptRemoteServerConfig(prompter, false)
                : await promptRemoteServerConfigSglang(prompter);
              baseUrl = `http://${(server as VllmServerConfig).host}:${(server as VllmServerConfig).port}/v1`;
            } else if (deploymentMethod === "docker") {
              server = provider === "vllm" 
                ? await promptDockerServerConfig(prompter, serverLocation === "remote")
                : await promptDockerServerConfigSglang(prompter, serverLocation === "remote");
              baseUrl = `http://${(server as VllmServerConfig).host}:${(server as VllmServerConfig).port}/v1`;
            } else {
              baseUrl = await promptBaseUrl(prompter, provider);
            }
          } else {
            baseUrl = await promptBaseUrl(prompter, provider);
          }
          
          const model = await promptModelName(prompter, provider);

          let gpuMemoryConfig: GpuMemoryConfig = {};
          if (provider === "vllm" || provider === "sglang") {
            gpuMemoryConfig = await promptGpuMemoryConfig(prompter);
          }

          const endpoint: ModelEndpoint = {
            provider: provider as any,
            baseUrl,
            model,
            server: server as any,
            gpuMemoryUtilization: gpuMemoryConfig.gpuMemoryUtilization,
            maxModelLen: gpuMemoryConfig.maxModelLen,
          };

          config = {
            id: label || `subagent-${Date.now()}`,
            name: name || "è‡ªå®šä¹‰å­æ™ºèƒ½ä½“",
            description,
            model: {
              endpoint,
            },
            behavior: {
              autoLoad: true,
              autoUnload: true,
              unloadDelayMs: 5000,
              temperature: 0.7,
              maxTokens: 4096,
            },
          };
        }

        config = await handleAIEnhancement(prompter, cfg, config);
        addSubagent(config);
        createSubagentWorkspaceFromConfig(config);

        await prompter.note(
          `å·²åˆ›å»ºå­æ™ºèƒ½ä½“: ${config.name}\næ¨¡å‹: ${config.model.endpoint.provider} - ${config.model.endpoint.model}`,
          "åˆ›å»ºæˆåŠŸ"
        );

        continueCreating = await prompter.confirm({
          message: "æ˜¯å¦ç»§ç»­åˆ›å»ºæ›´å¤šå­æ™ºèƒ½ä½“ï¼Ÿ",
          initialValue: true,
        });
      }
    }
  }

  await prompter.note(
    "å­æ™ºèƒ½ä½“é…ç½®å®Œæˆã€‚å¯ä½¿ç”¨ 'openclaw subagent list' æŸ¥çœ‹å’Œç®¡ç†ã€‚",
    "é…ç½®å®Œæˆ"
  );

  return cfg;
}
